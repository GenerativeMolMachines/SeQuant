{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SeQuant benchmarking notebook\n",
        "This notebook has been written for different embeddings strategies comparison on several benchmark datasets"
      ],
      "metadata": {
        "id": "ZCMUZbl7fzT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "n9wQVVWYi6tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libs import"
      ],
      "metadata": {
        "id": "9urY15VTgF_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUUMJJnvBZTR",
        "outputId": "b438b159-e79e-47a8-9011-df18ed08ec69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m140.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bzBQiidZfqRM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from Bio.Align import substitution_matrices\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark datasets import"
      ],
      "metadata": {
        "id": "09iwkqJH7K45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "antioxidative = pd.read_csv('antiox.csv')\n",
        "antiinflamatory = pd.read_csv('antiinf.csv')\n",
        "antimicrobial = pd.read_csv('antimic.csv')\n",
        "antidiabetic = pd.read_csv('antidia.csv')\n",
        "\n",
        "benchmark_list = [antioxidative, antiinflamatory, antimicrobial, antidiabetic]"
      ],
      "metadata": {
        "id": "M1Z0Vvq_fxrz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in benchmark_list:\n",
        "  dataset['length'] = dataset['seq'].apply(len)\n",
        "  length = dataset['length'].max()\n",
        "  print(length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJGtpEnS6bAo",
        "outputId": "9ee1e60b-4d0e-4195-c87a-c54bdfa4cb38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "30\n",
            "30\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence encoding"
      ],
      "metadata": {
        "id": "-zsxPb5w7YQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding functions"
      ],
      "metadata": {
        "id": "4jrethmEMMQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'"
      ],
      "metadata": {
        "id": "EcMKAIR2JpwU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(sequence):\n",
        "    encoder = OneHotEncoder(categories=[list(amino_acids)], dtype=int, sparse_output=False)\n",
        "    sequence_array = np.array(list(sequence)).reshape(-1, 1)\n",
        "    encoded = encoder.fit_transform(sequence_array).flatten()\n",
        "    return encoded\n",
        "\n",
        "\n",
        "def threemers_encode(sequence):\n",
        "    k = 3\n",
        "    kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
        "\n",
        "    kmer_to_index = {kmer: idx for idx, kmer in enumerate([''.join(p) for p in product(amino_acids, repeat=k)])}\n",
        "    encoded = [kmer_to_index[kmer] for kmer in kmers]\n",
        "    return encoded\n",
        "\n",
        "\n",
        "def blosum62_encode(sequence):\n",
        "    blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
        "    encoded_vector = []\n",
        "    for i in range(len(sequence) - 1):\n",
        "        pair = (sequence[i], sequence[i+1])\n",
        "        if pair in blosum62:\n",
        "            encoded_vector.append(blosum62[pair])\n",
        "        elif (pair[1], pair[0]) in blosum62:\n",
        "            encoded_vector.append(blosum62[(pair[1], pair[0])])\n",
        "        else:\n",
        "            encoded_vector.append(0)\n",
        "    return encoded_vector\n",
        "\n",
        "\n",
        "def process_dataset(df, encoding_func, encoding_name, pad_value):\n",
        "    encoded_data = df['seq'].apply(encoding_func)\n",
        "    max_len = max(encoded_data.apply(len))\n",
        "\n",
        "    encoded_data = encoded_data.apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=pad_value))\n",
        "\n",
        "    encoded_df = pd.DataFrame(encoded_data.tolist(), index=df.index)\n",
        "\n",
        "    result_df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "    return result_df\n",
        "\n"
      ],
      "metadata": {
        "id": "IPSN8FoM6h_j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions implementation"
      ],
      "metadata": {
        "id": "C-wPZyk1MS8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['antimic', 'antidia', 'antiinf', 'antiox']\n",
        "for dataset in datasets:\n",
        "    df = pd.read_csv(f'{dataset}.csv')\n",
        "\n",
        "    # One-hot encoding with padding 20\n",
        "    one_hot_df = process_dataset(df, one_hot_encode, 'one_hot', pad_value=0)\n",
        "    one_hot_df.to_csv(f'{dataset}_one_hot.csv', index=False)\n",
        "\n",
        "    # Threemers encoding with padding of max threemer index + 1\n",
        "    threemers_pad_value = len('ACDEFGHIKLMNPQRSTVWY') ** 3\n",
        "    threemers_df = process_dataset(df, threemers_encode, 'threemers', pad_value=threemers_pad_value)\n",
        "    threemers_df.to_csv(f'{dataset}_threemers.csv', index=False)\n",
        "\n",
        "    # BLOSUM62 encoding with padding 0\n",
        "    blosum62_df = process_dataset(df, blosum62_encode, 'blosum62', pad_value=0)\n",
        "    blosum62_df.to_csv(f'{dataset}_blosum62.csv', index=False)"
      ],
      "metadata": {
        "id": "5AjthgtWKX6e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SeQuant API usage for SeQuant embeddings"
      ],
      "metadata": {
        "id": "jqrGZko7P7oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    'accept': 'application/json',\n",
        "    'Content-Type': 'application/json',\n",
        "}\n",
        "\n",
        "for dataset in datasets:\n",
        "    df = pd.read_csv(f'{dataset}.csv')\n",
        "    sequences = list(df['seq'])\n",
        "\n",
        "    several_id_lists = np.array_split(np.asarray(sequences), int(len(sequences) / 50) + 1)\n",
        "\n",
        "    df_fin_data = pd.DataFrame()\n",
        "\n",
        "    for i in several_id_lists:\n",
        "        params = {\n",
        "            'sequences': ', '.join(list(i)),\n",
        "            'polymer_type': 'protein',\n",
        "            'encoding_strategy': 'protein',\n",
        "            'skip_unprocessable': 'true',\n",
        "        }\n",
        "\n",
        "        time.sleep(1)\n",
        "        response = requests.post('https://ai-chemistry.itmo.ru/api/encode_sequence', params=params, headers=headers)\n",
        "        assert response.status_code == 200\n",
        "        a = json.loads(response.content)\n",
        "        data = pd.DataFrame.from_dict(a, orient='index')\n",
        "        df_fin_data = pd.concat([df_fin_data, data])\n",
        "\n",
        "    df_fin_data['seq'] = df_fin_data.index\n",
        "\n",
        "    final_df = pd.merge(df, df_fin_data, on='seq', how='inner')\n",
        "\n",
        "    final_df.to_csv(f'{dataset}_sequant.csv', index=False)\n"
      ],
      "metadata": {
        "id": "PIkA4lLiRNiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGB classifier learning\n",
        "\n"
      ],
      "metadata": {
        "id": "4rFOyT_sWd7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libs import"
      ],
      "metadata": {
        "id": "EposImwZbv2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import joblib"
      ],
      "metadata": {
        "id": "V5xHMe6MTxm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model learning"
      ],
      "metadata": {
        "id": "nACHWC9zb3Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = ['antimic', 'antidia', 'antiinf', 'antiox']\n",
        "encoding_strategies = ['one_hot', 'threemers', 'blosum62', 'sequant']"
      ],
      "metadata": {
        "id": "64-MDlxrb2In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_list = []\n",
        "\n",
        "for task in tasks:\n",
        "    for encoding in encoding_strategies:\n",
        "\n",
        "        filename = f\"{task}_{encoding}.csv\"\n",
        "        df = pd.read_csv(filename)\n",
        "\n",
        "        descriptors = df.drop(columns=['seq', 'label'])\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_descriptors = scaler.fit_transform(descriptors)\n",
        "\n",
        "        # Save scaler\n",
        "        joblib.dump(scaler, f\"{task}_{encoding}_scaler.pkl\")\n",
        "\n",
        "        scaled_df = pd.DataFrame(scaled_descriptors, columns=descriptors.columns)\n",
        "        scaled_df['seq'] = df['seq']\n",
        "        scaled_df['label'] = df['label']\n",
        "\n",
        "        # Train_test split with stratification on label and random state = 11\n",
        "        train_df, test_df = train_test_split(scaled_df, test_size=0.2, stratify=scaled_df['label'], random_state=11)\n",
        "\n",
        "        model = XGBClassifier()\n",
        "\n",
        "        X_train = train_df.drop(columns=['seq', 'label'])\n",
        "        y_train = train_df['label']\n",
        "\n",
        "        # 5-fold cross validation\n",
        "        y_pred_cv = cross_val_predict(model, X_train, y_train, cv=5)\n",
        "\n",
        "        # Estimation on test\n",
        "        X_test = test_df.drop(columns=['seq', 'label'])\n",
        "        y_test = test_df['label']\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        # Evaluation\n",
        "        accuracy = accuracy_score(y_test, y_pred_test)\n",
        "        precision = precision_score(y_test, y_pred_test)\n",
        "        recall = recall_score(y_test, y_pred_test)\n",
        "        f1 = f1_score(y_test, y_pred_test)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_test)\n",
        "        mcc = matthews_corrcoef(y_test, y_pred_test)\n",
        "\n",
        "        cv_accuracy = accuracy_score(y_train, y_pred_cv)\n",
        "        cv_precision = precision_score(y_train, y_pred_cv)\n",
        "        cv_recall = recall_score(y_train, y_pred_cv)\n",
        "        cv_f1 = f1_score(y_train, y_pred_cv)\n",
        "        cv_roc_auc = roc_auc_score(y_train, y_pred_cv)\n",
        "        cv_mcc = matthews_corrcoef(y_train, y_pred_cv)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "\n",
        "        # Save model\n",
        "        model_filename = f\"{task}_{encoding}_model.pkl\"\n",
        "        joblib.dump(model, model_filename)\n",
        "\n",
        "        metrics_list.append({\n",
        "            'task': task,\n",
        "            'encoding_strategy': encoding,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'roc_auc': roc_auc,\n",
        "            'mcc': mcc,\n",
        "            'cv_accuracy': cv_accuracy,\n",
        "            'cv_precision': cv_precision,\n",
        "            'cv_recall': cv_recall,\n",
        "            'cv_f1_score': cv_f1,\n",
        "            'cv_roc_auc': cv_roc_auc,\n",
        "            'cv_mcc': cv_mcc,\n",
        "            'tn': tn,\n",
        "            'fp': fp,\n",
        "            'fn': fn,\n",
        "            'tp': tp\n",
        "        })\n",
        "\n",
        "# Create metrics df\n",
        "metrics_df = pd.DataFrame(metrics_list)\n",
        "metrics_df.to_csv(\"model_metrics.csv\", index=False)"
      ],
      "metadata": {
        "id": "FPQy6UG0b9sW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}